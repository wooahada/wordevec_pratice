목적

NLP기본인 Text Embedding 텍스트 임베딩을 공부하고 실습해본다.

목차

1. 학습 용어정리
2. 네이버 영화평 word2vec 만들기

-----------------------------------------------------------------

1. 학습 용어정리

단어집합(vocabulary) 
-> 문자를 숫자(벡터(vector))로 변경하는 One-hot encoding 원핫인코딩(결과 : 0 or 1) 사용 
-> 각 단어에 대해 고유한 index 부여 

예) 자연어(토큰 입력) -> [0, 0, 1, 0, 0, 0] (벡터값 출력)

원-핫 인코딩(One-hot encoding)의 한계 
1.단어가 100개면, 100개의 차원을 가진 벡터가 됨 
-> 단어의 값은 하나만 1, 나머지는 모두 0임. 
->메모리 저장공간에 있어 비효율적
2.단어의 유사도를 표현하지 못함

워드임베딩(Word Embedding)
단어를 밀집 벡터(dense vector)의 형태로 표현하는 방법
이 밀집 벡터를 워드 임베딩 과정을 통해 나온 결과라고 하여 임베딩 벡터(embedding vector)라고도 함

희소백터(sparse vector)
벡터 또는 행렬(matrix)의 값이 대부분이 0으로 표현되는 방법을 희소 표현(sparse representation) 혹은 희소백터(sparse vector)
원-핫 벡터는 희소 벡터(sparse vector)임
-> 단어간 유사도를 표현하지 못함

밀집 표현(dense representation) 
희소표현의 반대, 백터의 차원을 단어 집합의 크기로 산정하지 않음
사용자가 설정한 값으로 모든 단어의 백터 표현의 차원을 맞춰줌
0,1의 값이 아닌 실수값을 가지게 됨
벡터의 차원이 조밀해져서 밀집 백터(dense vector) 라고도 함

![image](https://github.com/user-attachments/assets/31a2e9e4-9be3-4db2-b20a-8ae146c7f192)

분산 표현(distributed representation)
단어간 유사성을 표현할 수 없다는 단점의 해결책
->단어의 '의미'를 다차원 공간에 벡터화하는 방법이 분산 표현(distributed representation)
'비슷한 위치에서 등장하는 단어들은 비슷한 의미를 가진다' 
분포 가설(distributional hypothesis)이라는 가정 하에 만들어진 표현 방법.

분산 표현을 이용하여 단어의 유사도를 벡터화하는 작업
=>워드 임베딩(embedding) 작업에 속함 
이렇게 표현된 벡터 또한 임베딩 벡터(embedding vector)라고 하며, 
저차원을 가지므로 밀집 벡터(dense vector)에도 속함

희소 표현이 고차원에 각 차원이 분리된 표현 방법 
분산 표현은 저차원에 단어의 의미를 여러 차원에 분산한 표현
->단어 간 유사도를 계산할 수 있음

------------------------------------------------------------------------

# wordevec_pratice
2. 네이버 영화평 word2vec 만들기

네이버에서 영화평을 쓴 사람들의 text의 데이터로 학습시킴
특정 단어를 입력하면 유사한 단어들을 보여줌

